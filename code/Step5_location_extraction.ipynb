{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ba8574-e72f-4371-8119-9ed775b71a4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.9.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: rapidfuzz\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed rapidfuzz-3.9.7\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install spacy==3.5.4\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d257c7-0821-40ea-ae5b-fbbfd85311f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import psutil\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rapidfuzz import process, fuzz\n",
    "import string\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === Clean paragraph text ===\n",
    "def clean_text(raw_text):\n",
    "    text = raw_text.replace('\\n', ' ')\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.replace('â€”', '-').replace('â€œ', '\"').replace('â€', '\"').replace(\"â€™\", \"'\")\n",
    "    text = re.sub(r'CHAPTER\\s[\\w\\d\\.]+.*?(?=\\s)', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'^[A-Z\\s\\.,\\'\\-]{10,}$', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "# === Extract cleaned sentences in batches ===\n",
    "def get_text(url, explorer_name, explorer_id, buffer_size=100):\n",
    "    url = url.strip()\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Bad Status: {e}, for url: {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    all_sentences = []\n",
    "    buffer = []\n",
    "\n",
    "    for tag in soup.find_all('p'):\n",
    "        content = tag.get_text(strip=True)\n",
    "        if content:\n",
    "            buffer.append(content)\n",
    "\n",
    "        if len(buffer) >= buffer_size:\n",
    "            chunk_text = ' '.join(buffer)\n",
    "            buffer = []\n",
    "\n",
    "            cleaned = clean_text(chunk_text)\n",
    "            sentences = sent_tokenize(cleaned)\n",
    "            all_sentences.extend(s for s in sentences if len(s.split()) >= 5)\n",
    "\n",
    "            print(f\"ğŸ§  Memory Used: {psutil.Process(os.getpid()).memory_info().rss / 1e6:.2f} MB\")\n",
    "\n",
    "    if buffer:\n",
    "        chunk_text = ' '.join(buffer)\n",
    "        cleaned = clean_text(chunk_text)\n",
    "        sentences = sent_tokenize(cleaned)\n",
    "        all_sentences.extend(s for s in sentences if len(s.split()) >= 5)\n",
    "\n",
    "    print(f\"âœ… Extracted {len(all_sentences)} sentences from {explorer_name} (ID: {explorer_id})\")\n",
    "    print(f\"â±ï¸ Time taken: {datetime.datetime.now() - now}\")\n",
    "\n",
    "    return all_sentences\n",
    "\n",
    "def match_sentence_to_paragraph_fast(sentence, paragraphs, threshold=90):\n",
    "    \"\"\"\n",
    "    Efficiently match a sentence to the best paragraph using RapidFuzz.\n",
    "    Returns best paragraph if similarity â‰¥ threshold, else empty string.\n",
    "    \"\"\"\n",
    "    best_match = process.extractOne(\n",
    "        sentence,\n",
    "        paragraphs,\n",
    "        scorer=fuzz.token_set_ratio\n",
    "    )\n",
    "    if best_match and best_match[1] >= threshold:\n",
    "        return best_match[0]\n",
    "    return \"\"\n",
    "\n",
    "def clean_for_matching(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Chunk paragraphs into blocks\n",
    "def chunk_paragraphs(paragraphs, block_size=100):\n",
    "    for i in range(0, len(paragraphs), block_size):\n",
    "        yield paragraphs[i:i + block_size]\n",
    "\n",
    "\n",
    "# Hybrid match: exact first, fallback to fuzzy search on blocks\n",
    "def match_sentence_to_paragraph_hybrid_optimized(\n",
    "    sentence, paragraph_map, cleaned_paragraphs, original_paragraphs,\n",
    "    threshold=90, block_size=100):\n",
    "    \n",
    "    # 1. Try exact match (original tokenized)\n",
    "    exact_match = paragraph_map.get(sentence.strip())\n",
    "    if exact_match:\n",
    "        return exact_match\n",
    "\n",
    "    # 2. Fuzzy match fallback\n",
    "    cleaned_sentence = clean_for_matching(sentence)\n",
    "    best_score = 0\n",
    "    best_para = \"\"\n",
    "\n",
    "    for block in chunk_paragraphs(cleaned_paragraphs, block_size):\n",
    "        match = process.extractOne(cleaned_sentence, block, scorer=fuzz.token_set_ratio)\n",
    "        if match and match[1] > best_score:\n",
    "            best_score = match[1]\n",
    "            best_para = match[0]\n",
    "\n",
    "    if best_score >= threshold:\n",
    "        idx = cleaned_paragraphs.index(best_para)\n",
    "        return original_paragraphs[idx]\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# === Extract location entities ===\n",
    "def extract_locations(text):\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}}\n",
    "    \n",
    "\n",
    "# === Main pipeline ===\n",
    "def enrich_classified_using_book_name_and_url(diary_url_csv, classified_csv, output_path=None, buffer_size=100, batch_size=5):\n",
    "    diary_url_df = pd.read_csv(diary_url_csv)\n",
    "    df_classified = pd.read_csv(classified_csv)\n",
    "    diary_url_df.columns = [col.lower() for col in diary_url_df.columns]\n",
    "    df_classified.columns = [col.lower() for col in df_classified.columns]\n",
    "\n",
    "    if 'sentence' not in df_classified.columns or 'book_name' not in df_classified.columns:\n",
    "        raise ValueError(\"Classified CSV must contain 'sentence' and 'book_name' columns.\")\n",
    "    if not {'explorer', 'diary', 'id'}.issubset(diary_url_df.columns):\n",
    "        raise ValueError(\"URL CSV must have 'explorer', 'diary', and 'id' columns.\")\n",
    "\n",
    "    all_results = []\n",
    "    explorers = df_classified['book_name'].dropna().unique()\n",
    "\n",
    "    for batch_start in range(0, len(explorers), batch_size):\n",
    "        batch_explorers = explorers[batch_start:batch_start + batch_size]\n",
    "\n",
    "        for explorer in batch_explorers:\n",
    "            start_dt = datetime.datetime.now()\n",
    "            print(f\"starting at :{start_dt}\")\n",
    "            match = diary_url_df[diary_url_df['explorer'].str.strip().str.upper() == explorer.strip().upper()]\n",
    "            if match.empty:\n",
    "                print(f\"âŒ No URL found for book_name: {explorer}\")\n",
    "                continue\n",
    "\n",
    "            url = match.iloc[0]['diary']\n",
    "            explorer_id = match.iloc[0]['id']\n",
    "            explorer_name = match.iloc[0]['explorer']\n",
    "            print(f\"\\nğŸ”— Processing {explorer_name} ({explorer_id}) from: {url}\")\n",
    "\n",
    "            parsed_sentences = get_text(url, explorer_name, explorer_id, buffer_size=buffer_size)\n",
    "            if not parsed_sentences:\n",
    "                continue\n",
    "\n",
    "            # Build sentence-to-paragraph map\n",
    "            paragraph_map = {}\n",
    "            for para in parsed_sentences:\n",
    "                for sent in sent_tokenize(para):\n",
    "                    if len(sent.split()) >= 5:\n",
    "                        paragraph_map[sent.strip()] = para\n",
    "\n",
    "            cleaned_paragraphs = [clean_for_matching(p) for p in parsed_sentences]\n",
    "\n",
    "            # Filter classified rows for this explorer\n",
    "            classified_subset = df_classified[df_classified['book_name'].str.strip().str.upper() == explorer.strip().upper()]\n",
    "\n",
    "            for _, row in classified_subset.iterrows():\n",
    "                sentence = row['sentence'].strip()\n",
    "\n",
    "                paragraph = match_sentence_to_paragraph_hybrid_optimized(\n",
    "                    sentence=sentence,\n",
    "                    paragraph_map=paragraph_map,\n",
    "                    cleaned_paragraphs=cleaned_paragraphs,\n",
    "                    original_paragraphs=parsed_sentences,\n",
    "                    threshold=90,\n",
    "                    block_size=100\n",
    "                )\n",
    "\n",
    "                loc_sent = extract_locations(sentence)\n",
    "                loc_para = extract_locations(paragraph)\n",
    "                para_only = loc_para - loc_sent\n",
    "\n",
    "                enriched = {\n",
    "                    **row.to_dict(),\n",
    "                    \"paragraph\": paragraph,\n",
    "                    \"locations_in_sentence\": list(loc_sent),\n",
    "                    \"locations_in_paragraph_only\": list(para_only),\n",
    "                    \"all_locations\": list(loc_sent.union(loc_para)),\n",
    "                    \"num_locations\": len(loc_sent.union(loc_para)),\n",
    "                    \"explorer_id\": explorer_id,\n",
    "                    \"explorer_name\": explorer_name,\n",
    "                    \"source_url\": url\n",
    "                }\n",
    "\n",
    "                all_results.append(enriched)\n",
    "\n",
    "            # === CLEAN UP after each explorer ===\n",
    "            print(f\"ğŸ§¹ Cleaning up memory for {explorer_name} (ID: {explorer_id})\")\n",
    "            del parsed_sentences, paragraph_map, cleaned_paragraphs, classified_subset\n",
    "            gc.collect()\n",
    "            print(f\"finishing at :{datetime.datetime.now()}, time took: {datetime.datetime.now() - start_dt}\")\n",
    "\n",
    "    df_out = pd.DataFrame(all_results)\n",
    "\n",
    "    if output_path:\n",
    "        df_out.to_csv(output_path, index=False)\n",
    "        print(f\"âœ… Saved enriched output to: {output_path}\")\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7400b8d-e414-42d1-bec8-23edb759e52e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at :2025-06-26 17:56:38.985682\n",
      "\n",
      "ğŸ”— Processing Edward John Eyre (10) from: https://gutenberg.net.au/ebooks/e00048.html \n",
      "ğŸ§  Memory Used: 3006.79 MB\n",
      "ğŸ§  Memory Used: 3318.54 MB\n",
      "ğŸ§  Memory Used: 3471.15 MB\n",
      "ğŸ§  Memory Used: 3553.23 MB\n",
      "ğŸ§  Memory Used: 3654.09 MB\n",
      "ğŸ§  Memory Used: 3734.97 MB\n",
      "ğŸ§  Memory Used: 3811.93 MB\n",
      "ğŸ§  Memory Used: 3869.72 MB\n",
      "ğŸ§  Memory Used: 3922.41 MB\n",
      "ğŸ§  Memory Used: 3993.79 MB\n",
      "ğŸ§  Memory Used: 4059.10 MB\n",
      "ğŸ§  Memory Used: 4102.46 MB\n",
      "ğŸ§  Memory Used: 4084.88 MB\n",
      "ğŸ§  Memory Used: 4181.56 MB\n",
      "ğŸ§  Memory Used: 4163.87 MB\n",
      "ğŸ§  Memory Used: 4239.10 MB\n",
      "ğŸ§  Memory Used: 4226.20 MB\n",
      "ğŸ§  Memory Used: 4215.98 MB\n",
      "ğŸ§  Memory Used: 4328.09 MB\n",
      "ğŸ§  Memory Used: 4278.16 MB\n",
      "ğŸ§  Memory Used: 4287.34 MB\n",
      "ğŸ§  Memory Used: 4291.52 MB\n",
      "âœ… Extracted 5521484 sentences from Edward John Eyre (ID: 10)\n",
      "â±ï¸ Time taken: 0:02:29.868631\n",
      "ğŸ§¹ Cleaning up memory for Edward John Eyre (ID: 10)\n",
      "finishing at :2025-06-26 18:00:52.152793, time took: 0:04:13.167131\n",
      "starting at :2025-06-26 18:00:52.153028\n",
      "\n",
      "ğŸ”— Processing Ludwig Leichhardt (11) from: https://gutenberg.net.au/ebooks/e00030.html \n",
      "ğŸ§  Memory Used: 2053.32 MB\n",
      "ğŸ§  Memory Used: 2121.44 MB\n",
      "ğŸ§  Memory Used: 2116.36 MB\n",
      "ğŸ§  Memory Used: 2168.92 MB\n",
      "ğŸ§  Memory Used: 2170.36 MB\n",
      "ğŸ§  Memory Used: 2217.33 MB\n",
      "ğŸ§  Memory Used: 2255.10 MB\n",
      "ğŸ§  Memory Used: 2283.75 MB\n",
      "ğŸ§  Memory Used: 2301.83 MB\n",
      "ğŸ§  Memory Used: 2310.22 MB\n",
      "âœ… Extracted 2003277 sentences from Ludwig Leichhardt (ID: 11)\n",
      "â±ï¸ Time taken: 0:00:45.969175\n",
      "ğŸ§¹ Cleaning up memory for Ludwig Leichhardt (ID: 11)\n",
      "finishing at :2025-06-26 18:04:33.354939, time took: 0:03:41.201921\n",
      "starting at :2025-06-26 18:04:33.355125\n",
      "\n",
      "ğŸ”— Processing Edmund B. Kennedy (13) from: https://gutenberg.net.au/ebooks13/1305191h.html \n",
      "âœ… Extracted 108 sentences from Edmund B. Kennedy (ID: 13)\n",
      "â±ï¸ Time taken: 0:00:00.956564\n",
      "ğŸ§¹ Cleaning up memory for Edmund B. Kennedy (ID: 13)\n",
      "finishing at :2025-06-26 18:04:34.477722, time took: 0:00:01.122612\n",
      "starting at :2025-06-26 18:04:34.477771\n",
      "\n",
      "ğŸ”— Processing A. C. Gregory (14) from: https://gutenberg.net.au/ebooks14/1402621h.html \n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "ğŸ§  Memory Used: 2074.87 MB\n",
      "âœ… Extracted 3442 sentences from A. C. Gregory (ID: 14)\n",
      "â±ï¸ Time taken: 0:00:01.844211\n",
      "ğŸ§¹ Cleaning up memory for A. C. Gregory (ID: 14)\n",
      "finishing at :2025-06-26 18:04:37.928605, time took: 0:00:03.450848\n",
      "starting at :2025-06-26 18:04:37.928651\n",
      "\n",
      "ğŸ”— Processing Major Warburton (15) from: https://gutenberg.net.au/ebooks12/1203701h.html \n",
      "âœ… Extracted 77 sentences from Major Warburton (ID: 15)\n",
      "â±ï¸ Time taken: 0:00:00.954909\n",
      "ğŸ§¹ Cleaning up memory for Major Warburton (ID: 15)\n",
      "finishing at :2025-06-26 18:04:39.402361, time took: 0:00:01.473725\n",
      "starting at :2025-06-26 18:04:39.402413\n",
      "\n",
      "ğŸ”— Processing Wentworth, Lawson, and Blaxland (1) from: https://gutenberg.net.au/ebooks02/0200411h.html#blax1 \n",
      "âœ… Extracted 307 sentences from Wentworth, Lawson, and Blaxland (ID: 1)\n",
      "â±ï¸ Time taken: 0:00:00.999802\n",
      "ğŸ§¹ Cleaning up memory for Wentworth, Lawson, and Blaxland (ID: 1)\n",
      "finishing at :2025-06-26 18:04:40.555993, time took: 0:00:01.153595\n",
      "starting at :2025-06-26 18:04:40.556041\n",
      "\n",
      "ğŸ”— Processing McDouall Stuart (20) from: https://gutenberg.net.au/ebooks/e00040.html \n",
      "ğŸ§  Memory Used: 2075.08 MB\n",
      "ğŸ§  Memory Used: 2144.07 MB\n",
      "ğŸ§  Memory Used: 2144.69 MB\n",
      "ğŸ§  Memory Used: 2223.33 MB\n",
      "ğŸ§  Memory Used: 2287.13 MB\n",
      "ğŸ§  Memory Used: 2338.86 MB\n",
      "ğŸ§  Memory Used: 2377.94 MB\n",
      "ğŸ§  Memory Used: 2403.73 MB\n",
      "ğŸ§  Memory Used: 2414.42 MB\n",
      "ğŸ§  Memory Used: 2416.99 MB\n",
      "ğŸ§  Memory Used: 2417.78 MB\n",
      "ğŸ§  Memory Used: 2417.97 MB\n",
      "ğŸ§  Memory Used: 2417.97 MB\n",
      "ğŸ§  Memory Used: 2417.97 MB\n",
      "ğŸ§  Memory Used: 2417.97 MB\n",
      "âœ… Extracted 3283106 sentences from McDouall Stuart (ID: 20)\n",
      "â±ï¸ Time taken: 0:01:09.456828\n",
      "ğŸ§¹ Cleaning up memory for McDouall Stuart (ID: 20)\n",
      "finishing at :2025-06-26 18:06:26.324789, time took: 0:01:45.768766\n",
      "starting at :2025-06-26 18:06:26.324844\n",
      "\n",
      "ğŸ”— Processing Burke & Wills (21) from: https://gutenberg.net.au/ebooks/e00060.html \n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.15 MB\n",
      "ğŸ§  Memory Used: 2098.45 MB\n",
      "ğŸ§  Memory Used: 2104.05 MB\n",
      "ğŸ§  Memory Used: 2106.75 MB\n",
      "ğŸ§  Memory Used: 2107.59 MB\n",
      "âœ… Extracted 1163849 sentences from Burke & Wills (ID: 21)\n",
      "â±ï¸ Time taken: 0:00:30.131738\n",
      "ğŸ§¹ Cleaning up memory for Burke & Wills (ID: 21)\n",
      "finishing at :2025-06-26 18:07:17.496392, time took: 0:00:51.171565\n",
      "starting at :2025-06-26 18:07:17.496450\n",
      "\n",
      "ğŸ”— Processing In search of Burke & Willsâ€”Frederick Walker (22) from: https://gutenberg.net.au/ebooks/e00029.html \n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "ğŸ§  Memory Used: 2101.79 MB\n",
      "âœ… Extracted 2369 sentences from In search of Burke & Willsâ€”Frederick Walker (ID: 22)\n",
      "â±ï¸ Time taken: 0:00:01.535028\n",
      "ğŸ§¹ Cleaning up memory for In search of Burke & Willsâ€”Frederick Walker (ID: 22)\n",
      "finishing at :2025-06-26 18:07:19.926206, time took: 0:00:02.429771\n",
      "starting at :2025-06-26 18:07:19.926252\n",
      "\n",
      "ğŸ”— Processing Frank & Alexander Jardine (23) from: https://gutenberg.net.au/ebooks/e00026.html \n",
      "ğŸ§  Memory Used: 2101.30 MB\n",
      "ğŸ§  Memory Used: 2101.30 MB\n",
      "âœ… Extracted 250284 sentences from Frank & Alexander Jardine (ID: 23)\n",
      "â±ï¸ Time taken: 0:00:06.464357\n",
      "ğŸ§¹ Cleaning up memory for Frank & Alexander Jardine (ID: 23)\n",
      "finishing at :2025-06-26 18:07:30.414943, time took: 0:00:10.488708\n",
      "starting at :2025-06-26 18:07:30.415003\n",
      "\n",
      "ğŸ”— Processing John & A. Forrest (24) from: https://gutenberg.net.au/ebooks/e00051.html \n",
      "ğŸ§  Memory Used: 2101.30 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "ğŸ§  Memory Used: 2101.31 MB\n",
      "âœ… Extracted 1173123 sentences from John & A. Forrest (ID: 24)\n",
      "â±ï¸ Time taken: 0:00:27.211321\n",
      "ğŸ§¹ Cleaning up memory for John & A. Forrest (ID: 24)\n",
      "finishing at :2025-06-26 18:08:15.122983, time took: 0:00:44.707996\n",
      "starting at :2025-06-26 18:08:15.123036\n",
      "\n",
      "ğŸ”— Processing Ernest Giles (25) from: https://gutenberg.net.au/ebooks/e00052.html \n",
      "ğŸ§  Memory Used: 2524.39 MB\n",
      "ğŸ§  Memory Used: 2302.91 MB\n",
      "ğŸ§  Memory Used: 2370.78 MB\n",
      "ğŸ§  Memory Used: 2561.29 MB\n",
      "ğŸ§  Memory Used: 2464.33 MB\n",
      "ğŸ§  Memory Used: 2406.87 MB\n",
      "ğŸ§  Memory Used: 2432.61 MB\n",
      "âœ… Extracted 2816804 sentences from Ernest Giles (ID: 25)\n",
      "â±ï¸ Time taken: 0:01:09.582675\n",
      "ğŸ§¹ Cleaning up memory for Ernest Giles (ID: 25)\n",
      "finishing at :2025-06-26 18:10:06.112196, time took: 0:01:50.989179\n",
      "starting at :2025-06-26 18:10:06.112250\n",
      "\n",
      "ğŸ”— Processing W.G. Gosse (27) from: https://gutenberg.net.au/ebooks13/1306451h.html \n",
      "ğŸ§  Memory Used: 2135.53 MB\n",
      "ğŸ§  Memory Used: 2135.53 MB\n",
      "âœ… Extracted 1294 sentences from W.G. Gosse (ID: 27)\n",
      "â±ï¸ Time taken: 0:00:01.263975\n",
      "ğŸ§¹ Cleaning up memory for W.G. Gosse (ID: 27)\n",
      "finishing at :2025-06-26 18:10:07.662256, time took: 0:00:01.550018\n",
      "starting at :2025-06-26 18:10:07.662294\n",
      "\n",
      "ğŸ”— Processing Surveyor G. W. Evans (2) from: https://gutenberg.net.au/ebooks13/1300271h.html \n",
      "ğŸ§  Memory Used: 2134.74 MB\n",
      "ğŸ§  Memory Used: 2134.74 MB\n",
      "ğŸ§  Memory Used: 2134.74 MB\n",
      "ğŸ§  Memory Used: 2134.74 MB\n",
      "âœ… Extracted 701 sentences from Surveyor G. W. Evans (ID: 2)\n",
      "â±ï¸ Time taken: 0:00:01.278531\n",
      "ğŸ§¹ Cleaning up memory for Surveyor G. W. Evans (ID: 2)\n",
      "finishing at :2025-06-26 18:10:09.204803, time took: 0:00:01.542516\n",
      "starting at :2025-06-26 18:10:09.204972\n",
      "\n",
      "ğŸ”— Processing D. Lindsay (31) from: https://gutenberg.net.au/ebooks12/1203431h.html \n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "âœ… Extracted 5417 sentences from D. Lindsay (ID: 31)\n",
      "â±ï¸ Time taken: 0:00:01.899959\n",
      "ğŸ§¹ Cleaning up memory for D. Lindsay (ID: 31)\n",
      "finishing at :2025-06-26 18:10:12.335706, time took: 0:00:03.130745\n",
      "starting at :2025-06-26 18:10:12.335750\n",
      "\n",
      "ğŸ”— Processing Hon. D.C. Carnegie (32) from: https://gutenberg.net.au/ebooks/e00042.html \n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "âœ… Extracted 4252 sentences from Hon. D.C. Carnegie (ID: 32)\n",
      "â±ï¸ Time taken: 0:00:01.823195\n",
      "ğŸ§¹ Cleaning up memory for Hon. D.C. Carnegie (ID: 32)\n",
      "finishing at :2025-06-26 18:10:15.821596, time took: 0:00:03.485861\n",
      "starting at :2025-06-26 18:10:15.821642\n",
      "\n",
      "ğŸ”— Processing Surveyor General John Oxley (3) from: https://gutenberg.net.au/ebooks/e00037.html \n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "âœ… Extracted 2682 sentences from Surveyor General John Oxley (ID: 3)\n",
      "â±ï¸ Time taken: 0:00:01.527001\n",
      "ğŸ§¹ Cleaning up memory for Surveyor General John Oxley (ID: 3)\n",
      "finishing at :2025-06-26 18:10:18.388253, time took: 0:00:02.566625\n",
      "starting at :2025-06-26 18:10:18.388299\n",
      "\n",
      "ğŸ”— Processing Allan Cunningham (4) from: https://gutenberg.net.au/ebooks13/1304431h.html \n",
      "âœ… Extracted 309 sentences from Allan Cunningham (ID: 4)\n",
      "â±ï¸ Time taken: 0:00:01.202654\n",
      "ğŸ§¹ Cleaning up memory for Allan Cunningham (ID: 4)\n",
      "finishing at :2025-06-26 18:10:19.729388, time took: 0:00:01.341104\n",
      "starting at :2025-06-26 18:10:19.729436\n",
      "\n",
      "ğŸ”— Processing Hume and Hovell (5) from: https://gutenberg.net.au/ebooks04/0400371h.html \n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "âœ… Extracted 999 sentences from Hume and Hovell (ID: 5)\n",
      "â±ï¸ Time taken: 0:00:01.242649\n",
      "ğŸ§¹ Cleaning up memory for Hume and Hovell (ID: 5)\n",
      "finishing at :2025-06-26 18:10:21.309998, time took: 0:00:01.580577\n",
      "starting at :2025-06-26 18:10:21.310048\n",
      "\n",
      "ğŸ”— Processing Capt. Chas. Sturt (6) from: https://gutenberg.net.au/ebooks07/0700391h.html#ch10 \n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "ğŸ§  Memory Used: 2134.48 MB\n",
      "âœ… Extracted 2484 sentences from Capt. Chas. Sturt (ID: 6)\n",
      "â±ï¸ Time taken: 0:00:01.596006\n",
      "ğŸ§¹ Cleaning up memory for Capt. Chas. Sturt (ID: 6)\n",
      "finishing at :2025-06-26 18:10:23.628658, time took: 0:00:02.318623\n",
      "starting at :2025-06-26 18:10:23.628709\n",
      "\n",
      "ğŸ”— Processing Sir Thomas Mitchell (7) from: https://gutenberg.net.au/ebooks/e00035.html \n",
      "ğŸ§  Memory Used: 2134.60 MB\n",
      "ğŸ§  Memory Used: 2134.60 MB\n",
      "ğŸ§  Memory Used: 2134.60 MB\n",
      "ğŸ§  Memory Used: 2193.47 MB\n",
      "ğŸ§  Memory Used: 2134.62 MB\n",
      "ğŸ§  Memory Used: 2165.53 MB\n",
      "ğŸ§  Memory Used: 2217.74 MB\n",
      "ğŸ§  Memory Used: 2263.90 MB\n",
      "ğŸ§  Memory Used: 2306.76 MB\n",
      "ğŸ§  Memory Used: 2346.95 MB\n",
      "ğŸ§  Memory Used: 2385.47 MB\n",
      "ğŸ§  Memory Used: 2419.61 MB\n",
      "ğŸ§  Memory Used: 2451.55 MB\n",
      "ğŸ§  Memory Used: 2479.86 MB\n",
      "ğŸ§  Memory Used: 2504.24 MB\n",
      "ğŸ§  Memory Used: 2521.81 MB\n",
      "ğŸ§  Memory Used: 2536.64 MB\n",
      "ğŸ§  Memory Used: 2546.66 MB\n",
      "ğŸ§  Memory Used: 2552.41 MB\n",
      "ğŸ§  Memory Used: 2554.04 MB\n",
      "âœ… Extracted 2968052 sentences from Sir Thomas Mitchell (ID: 7)\n",
      "â±ï¸ Time taken: 0:01:11.286800\n"
     ]
    }
   ],
   "source": [
    "df = enrich_classified_using_book_name_and_url(\n",
    "    diary_url_csv=\"/cluster/home/hlee37/git/gutenberg/phase2/DataForJohn.csv\",\n",
    "    classified_csv=\"/cluster/home/hlee37/git/gutenberg/phase4/output/filtered_sentences_0.csv\",\n",
    "    output_path=\"all_with_locations_2.csv\",\n",
    "    buffer_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193cb59-7907-475b-a29a-adc8f8d81ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8fc2f-0d17-4066-8bfb-ac65e244d02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a7d8f9-bea0-4df6-b3b5-ccccfee82b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/hlee37/condaenv/torch_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# === Location extractor ===\n",
    "def extract_locations(text):\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\", \"FAC\"}}\n",
    "\n",
    "# === Main function ===\n",
    "def enrich_classified_with_locations(classified_csv, output_path=None):\n",
    "    df = pd.read_csv(classified_csv)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    if 'sentence' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain a 'sentence' column.\")\n",
    "\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence'].strip()\n",
    "        locations = extract_locations(sentence)\n",
    "\n",
    "        enriched = {\n",
    "            **row.to_dict(),\n",
    "            \"locations_in_sentence\": list(locations),\n",
    "            \"num_locations\": len(locations)\n",
    "        }\n",
    "\n",
    "        results.append(enriched)\n",
    "\n",
    "    df_out = pd.DataFrame(results)\n",
    "\n",
    "    if output_path:\n",
    "        df_out.to_csv(output_path, index=False)\n",
    "        print(f\"âœ… Saved output to {output_path}\")\n",
    "\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbaecc7-51da-43a0-8869-7aca66688252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved output to locations_output.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos_prob</th>\n",
       "      <th>neg_prob</th>\n",
       "      <th>book_name</th>\n",
       "      <th>locations_in_sentence</th>\n",
       "      <th>num_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In offering to the public an account of Expedi...</td>\n",
       "      <td>0.905523</td>\n",
       "      <td>0.094477</td>\n",
       "      <td>Edward John Eyre</td>\n",
       "      <td>[Australia, England, South Australia, the Murr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33 degrees 14 minutes S, is a practicable pass...</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999888</td>\n",
       "      <td>Edward John Eyre</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By digging in the gravelly bed of the channel,...</td>\n",
       "      <td>0.295373</td>\n",
       "      <td>0.704627</td>\n",
       "      <td>Edward John Eyre</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In descending, I followed a little rocky gully...</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>Edward John Eyre</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We had been so long living upon nothing but th...</td>\n",
       "      <td>0.999568</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>Edward John Eyre</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>The combat now became general--spears flew in ...</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>Sir Thomas Mitchell</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>It was now seen that further resistance would ...</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>Sir Thomas Mitchell</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>When attempting to make their escape a line wa...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>Sir Thomas Mitchell</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>During their stay every precaution was taken b...</td>\n",
       "      <td>0.893572</td>\n",
       "      <td>0.106428</td>\n",
       "      <td>Sir Thomas Mitchell</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>So friendly did they appear, that neither the ...</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.999909</td>\n",
       "      <td>Sir Thomas Mitchell</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3115 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  pos_prob  neg_prob  \\\n",
       "0     In offering to the public an account of Expedi...  0.905523  0.094477   \n",
       "1     33 degrees 14 minutes S, is a practicable pass...  0.000112  0.999888   \n",
       "2     By digging in the gravelly bed of the channel,...  0.295373  0.704627   \n",
       "3     In descending, I followed a little rocky gully...  0.999435  0.000565   \n",
       "4     We had been so long living upon nothing but th...  0.999568  0.000432   \n",
       "...                                                 ...       ...       ...   \n",
       "3110  The combat now became general--spears flew in ...  0.000052  0.999948   \n",
       "3111  It was now seen that further resistance would ...  0.000068  0.999932   \n",
       "3112  When attempting to make their escape a line wa...  0.000060  0.999940   \n",
       "3113  During their stay every precaution was taken b...  0.893572  0.106428   \n",
       "3114  So friendly did they appear, that neither the ...  0.000091  0.999909   \n",
       "\n",
       "                book_name                              locations_in_sentence  \\\n",
       "0        Edward John Eyre  [Australia, England, South Australia, the Murr...   \n",
       "1        Edward John Eyre                                                 []   \n",
       "2        Edward John Eyre                                                 []   \n",
       "3        Edward John Eyre                                                 []   \n",
       "4        Edward John Eyre                                                 []   \n",
       "...                   ...                                                ...   \n",
       "3110  Sir Thomas Mitchell                                                 []   \n",
       "3111  Sir Thomas Mitchell                                                 []   \n",
       "3112  Sir Thomas Mitchell                                                 []   \n",
       "3113  Sir Thomas Mitchell                                                 []   \n",
       "3114  Sir Thomas Mitchell                                                 []   \n",
       "\n",
       "      num_locations  \n",
       "0                 4  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "3110              0  \n",
       "3111              0  \n",
       "3112              0  \n",
       "3113              0  \n",
       "3114              0  \n",
       "\n",
       "[3115 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrich_classified_with_locations(\"/cluster/home/hlee37/git/gutenberg/phase4/output/filtered_sentences_0.csv\", output_path=\"locations_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfae38b-52a5-46e0-b0dc-167cf8f5de4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
